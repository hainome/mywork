{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR回路をNNで解く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osaka/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import links as L\n",
    "from chainer import initializer as I\n",
    "from chainer import functions as F\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "print(chainer.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: <chainer.datasets.tuple_dataset.TupleDataset object at 0x7ff981f600b8>\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.693147    0.692648              0.25           0.75                      0.00337495    \n",
      "\u001b[J2           0.692648    0.692149              0.75           0.75                      0.00698262    \n",
      "\u001b[J3           0.692149    0.691652              0.75           0.75                      0.0497372     \n",
      "\u001b[J4           0.691652    0.691155              0.75           0.75                      0.0623164     \n",
      "\u001b[J5           0.691155    0.69066               0.75           0.75                      0.0696947     \n",
      "\u001b[J6           0.69066     0.690166              0.75           0.75                      0.092315      \n",
      "\u001b[J7           0.690166    0.689673              0.75           0.75                      0.101835      \n",
      "\u001b[J8           0.689673    0.689181              0.75           0.75                      0.108736      \n",
      "\u001b[J9           0.689181    0.68869               0.75           0.75                      0.125729      \n",
      "\u001b[J10          0.68869     0.6882                0.75           0.75                      0.131642      \n",
      "\u001b[J11          0.6882      0.687712              0.75           0.75                      0.1369        \n",
      "\u001b[J12          0.687712    0.687225              0.75           0.75                      0.141086      \n",
      "\u001b[J13          0.687225    0.686738              0.75           0.75                      0.144794      \n",
      "\u001b[J14          0.686739    0.686254              0.75           0.75                      0.14832       \n",
      "\u001b[J15          0.686254    0.68577               0.75           0.75                      0.152001      \n",
      "\u001b[J16          0.68577     0.685287              0.75           0.75                      0.155573      \n",
      "\u001b[J17          0.685287    0.684806              0.75           0.75                      0.159275      \n",
      "\u001b[J18          0.684806    0.684326              0.75           0.75                      0.162813      \n",
      "\u001b[J19          0.684326    0.683848              0.75           0.75                      0.166545      \n",
      "\u001b[J20          0.683848    0.68337               0.75           0.75                      0.170254      \n",
      "\u001b[J21          0.68337     0.682894              0.75           0.75                      0.173972      \n",
      "\u001b[J22          0.682894    0.68242               0.75           0.75                      0.177694      \n",
      "\u001b[J23          0.68242     0.681946              0.75           0.75                      0.181462      \n",
      "\u001b[J24          0.681946    0.681474              0.75           0.75                      0.185192      \n",
      "\u001b[J25          0.681474    0.681003              0.75           0.75                      0.189026      \n",
      "\u001b[J26          0.681003    0.680534              0.75           0.75                      0.192812      \n",
      "\u001b[J27          0.680534    0.680066              0.75           0.75                      0.196657      \n",
      "\u001b[J28          0.680066    0.679599              0.75           0.75                      0.200534      \n",
      "\u001b[J29          0.679599    0.679133              0.75           0.75                      0.205763      \n",
      "\u001b[J30          0.679133    0.678669              0.75           0.75                      0.210354      \n",
      "\u001b[J31          0.678669    0.678207              0.75           0.75                      0.217405      \n",
      "\u001b[J32          0.678207    0.677745              0.75           0.75                      0.221606      \n",
      "\u001b[J33          0.677745    0.677285              0.75           0.75                      0.225135      \n",
      "\u001b[J34          0.677285    0.676827              0.75           0.75                      0.228589      \n",
      "\u001b[J35          0.676827    0.676369              0.75           0.75                      0.234755      \n",
      "\u001b[J36          0.676369    0.675914              0.75           0.75                      0.238989      \n",
      "\u001b[J37          0.675914    0.675459              0.75           0.75                      0.242892      \n",
      "\u001b[J38          0.675459    0.675006              0.75           0.75                      0.246374      \n",
      "\u001b[J39          0.675006    0.674554              0.75           0.75                      0.250491      \n",
      "\u001b[J40          0.674554    0.674104              0.75           0.75                      0.254333      \n",
      "\u001b[J41          0.674104    0.673655              0.75           0.75                      0.258034      \n",
      "\u001b[J42          0.673655    0.673208              0.75           0.75                      0.261821      \n",
      "\u001b[J43          0.673208    0.672762              0.75           0.75                      0.26541       \n",
      "\u001b[J44          0.672762    0.672317              0.75           0.75                      0.26935       \n",
      "\u001b[J45          0.672317    0.671874              0.75           0.75                      0.273198      \n",
      "\u001b[J46          0.671874    0.671431              0.75           0.75                      0.277078      \n",
      "\u001b[J47          0.671431    0.670991              0.75           0.75                      0.28079       \n",
      "\u001b[J48          0.670991    0.670552              0.75           0.75                      0.284883      \n",
      "\u001b[J49          0.670552    0.670114              0.75           0.75                      0.288714      \n",
      "\u001b[J50          0.670114    0.669677              0.75           0.75                      0.292593      \n",
      "\u001b[J51          0.669677    0.669243              0.75           0.75                      0.296422      \n",
      "\u001b[J52          0.669243    0.668809              0.75           0.75                      0.300534      \n",
      "\u001b[J53          0.668809    0.668377              0.75           0.75                      0.306224      \n",
      "\u001b[J54          0.668377    0.667946              0.75           0.75                      0.313235      \n",
      "\u001b[J55          0.667946    0.667516              0.75           0.75                      0.32022       \n",
      "\u001b[J56          0.667516    0.667088              0.75           0.75                      0.326197      \n",
      "\u001b[J57          0.667088    0.666661              0.75           0.75                      0.330077      \n",
      "\u001b[J58          0.666661    0.666236              0.75           0.75                      0.334318      \n",
      "\u001b[J59          0.666236    0.665812              0.75           0.75                      0.338346      \n",
      "\u001b[J60          0.665812    0.665389              0.75           0.75                      0.342434      \n",
      "\u001b[J61          0.665389    0.664968              0.75           0.75                      0.34656       \n",
      "\u001b[J62          0.664968    0.664548              0.75           0.75                      0.350715      \n",
      "\u001b[J63          0.664548    0.66413               0.75           0.75                      0.354841      \n",
      "\u001b[J64          0.66413     0.663713              0.75           0.75                      0.358994      \n",
      "\u001b[J65          0.663713    0.663297              0.75           0.75                      0.363239      \n",
      "\u001b[J66          0.663297    0.662882              0.75           0.75                      0.367419      \n",
      "\u001b[J67          0.662882    0.662469              0.75           0.75                      0.37153       \n",
      "\u001b[J68          0.662469    0.662058              0.75           0.75                      0.3755        \n",
      "\u001b[J69          0.662058    0.661647              0.75           0.75                      0.379888      \n",
      "\u001b[J70          0.661647    0.661238              0.75           0.75                      0.383856      \n",
      "\u001b[J71          0.661238    0.660831              0.75           0.75                      0.388296      \n",
      "\u001b[J72          0.660831    0.660424              0.75           0.75                      0.392616      \n",
      "\u001b[J73          0.660424    0.660019              0.75           0.75                      0.39688       \n",
      "\u001b[J74          0.660019    0.659616              0.75           0.75                      0.400953      \n",
      "\u001b[J75          0.659616    0.659213              0.75           0.75                      0.406895      \n",
      "\u001b[J76          0.659213    0.658812              0.75           0.75                      0.414107      \n",
      "\u001b[J77          0.658812    0.658413              0.75           0.75                      0.422698      \n",
      "\u001b[J78          0.658413    0.658014              0.75           0.75                      0.427703      \n",
      "\u001b[J79          0.658014    0.657617              0.75           0.75                      0.434091      \n",
      "\u001b[J80          0.657617    0.657222              0.75           0.75                      0.438617      \n",
      "\u001b[J81          0.657222    0.656828              0.75           0.75                      0.442947      \n",
      "\u001b[J82          0.656828    0.656434              0.75           0.75                      0.447695      \n",
      "\u001b[J83          0.656434    0.656043              0.75           0.75                      0.452064      \n",
      "\u001b[J84          0.656043    0.655652              0.75           0.75                      0.456693      \n",
      "\u001b[J85          0.655652    0.655263              0.75           0.75                      0.461284      \n",
      "\u001b[J86          0.655263    0.654875              0.75           0.75                      0.465917      \n",
      "\u001b[J87          0.654875    0.654489              0.75           0.75                      0.470383      \n",
      "\u001b[J88          0.654489    0.654104              0.75           0.75                      0.474866      \n",
      "\u001b[J89          0.654104    0.65372               0.75           0.75                      0.479465      \n",
      "\u001b[J90          0.65372     0.653337              0.75           0.75                      0.484087      \n",
      "\u001b[J91          0.653337    0.652956              0.75           0.75                      0.488619      \n",
      "\u001b[J92          0.652956    0.652576              0.75           0.75                      0.493102      \n",
      "\u001b[J93          0.652576    0.652197              0.75           0.75                      0.497687      \n",
      "\u001b[J94          0.652197    0.65182               0.75           0.75                      0.502238      \n",
      "\u001b[J95          0.65182     0.651444              0.75           0.75                      0.506959      \n",
      "\u001b[J96          0.651444    0.651069              0.75           0.75                      0.511401      \n",
      "\u001b[J97          0.651069    0.650695              0.75           0.75                      0.51668       \n",
      "\u001b[J98          0.650695    0.650323              0.75           0.75                      0.521708      \n",
      "\u001b[J99          0.650323    0.649952              0.75           0.75                      0.526901      \n",
      "\u001b[J100         0.649952    0.649582              0.75           0.75                      0.532069      \n"
     ]
    }
   ],
   "source": [
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(2, 3)    #入力層２、中間層３\n",
    "            self.l2 = L.Linear(3, 2)    #中間層３，出力層２\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))    #ReLU function\n",
    "        y = self.l2(h1)\n",
    "        return y\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 4\n",
    "\n",
    "#データの作成\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_train = np.array([0, 1, 1, 1], dtype=np.int32)\n",
    "train = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "test = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "print(\"train:\", train)\n",
    "\n",
    "#ニューラルネットワークの登録\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializer.load_npz(\"results/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "#イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batch_size)    #学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)    #評価用\n",
    "\n",
    "#アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "#トレーナの保存\n",
    "trainer = training.Trainer(updater, (epoch, \"epoch\"))\n",
    "\n",
    "#学習状況の保存や表示\n",
    "trainer.extend(extensions.LogReport())    #ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))    #エポック数の表示\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \n",
    "                                       \"main/accuracy\", \"validation/main/accuracy\", \"elapsed_time\"]))    #計算状態の表示\n",
    "#trainer.extend(extentions.dump_graph(\"main/loss\"))    #NNの構造\n",
    "#trainer.extend(extensions.PlotReport[\"main/loss\", \"validation/main/loss\"], \"epoch\", file_name=\"loss.png\")    #誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport[\"main/accuracy\", \"validation/main/accuracy\"], \"epoch\", file_name=\"accuracy.png\")    #精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, \"epoch\"))    #学習再開のためのスナップショットの出力\n",
    "\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)    #学習再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\")\n",
    "\n",
    "#学習開始\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パーセプトロンをNNで解く<br>\n",
    "MyChainを変えればネットワークを改造できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: <chainer.datasets.tuple_dataset.TupleDataset object at 0x7ff981f4f400>\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.790447    0.788727              0.25           0                         0.00317841    \n",
      "\u001b[J2           0.788727    0.787011              0              0                         0.00885194    \n",
      "\u001b[J3           0.787011    0.7853                0              0                         0.051841      \n",
      "\u001b[J4           0.7853      0.783594              0              0                         0.0594085     \n",
      "\u001b[J5           0.783594    0.781893              0              0                         0.0649229     \n",
      "\u001b[J6           0.781893    0.780196              0              0                         0.0700113     \n",
      "\u001b[J7           0.780196    0.778504              0              0.25                      0.0739901     \n",
      "\u001b[J8           0.778504    0.776817              0.25           0.25                      0.0828812     \n",
      "\u001b[J9           0.776817    0.775136              0.25           0.25                      0.0853212     \n",
      "\u001b[J10          0.775136    0.773459              0.25           0.25                      0.0912562     \n",
      "\u001b[J11          0.773459    0.771788              0.25           0.25                      0.0942319     \n",
      "\u001b[J12          0.771788    0.770121              0.25           0.25                      0.0969401     \n",
      "\u001b[J13          0.770121    0.768461              0.25           0.25                      0.0995376     \n",
      "\u001b[J14          0.768461    0.766805              0.25           0.25                      0.102447      \n",
      "\u001b[J15          0.766805    0.765155              0.25           0.25                      0.105222      \n",
      "\u001b[J16          0.765155    0.76351               0.25           0.25                      0.108148      \n",
      "\u001b[J17          0.763511    0.761871              0.25           0.25                      0.110749      \n",
      "\u001b[J18          0.761871    0.760238              0.25           0.25                      0.113514      \n",
      "\u001b[J19          0.760238    0.75861               0.25           0.25                      0.116529      \n",
      "\u001b[J20          0.75861     0.756988              0.25           0.25                      0.119251      \n",
      "\u001b[J21          0.756988    0.755372              0.25           0.25                      0.122045      \n",
      "\u001b[J22          0.755372    0.753761              0.25           0.25                      0.124866      \n",
      "\u001b[J23          0.753761    0.752156              0.25           0.25                      0.127925      \n",
      "\u001b[J24          0.752156    0.750557              0.25           0.25                      0.130801      \n",
      "\u001b[J25          0.750557    0.748964              0.25           0.25                      0.133553      \n",
      "\u001b[J26          0.748964    0.747377              0.25           0.25                      0.136509      \n",
      "\u001b[J27          0.747377    0.745796              0.25           0.25                      0.139642      \n",
      "\u001b[J28          0.745796    0.744221              0.25           0.25                      0.142645      \n",
      "\u001b[J29          0.744221    0.742651              0.25           0.25                      0.145441      \n",
      "\u001b[J30          0.742651    0.741088              0.25           0.25                      0.148687      \n",
      "\u001b[J31          0.741088    0.739531              0.25           0.25                      0.151879      \n",
      "\u001b[J32          0.739531    0.73798               0.25           0.25                      0.155103      \n",
      "\u001b[J33          0.73798     0.736435              0.25           0.25                      0.158352      \n",
      "\u001b[J34          0.736435    0.734896              0.25           0.25                      0.161597      \n",
      "\u001b[J35          0.734896    0.733363              0.25           0.25                      0.165147      \n",
      "\u001b[J36          0.733363    0.731836              0.25           0.25                      0.168424      \n",
      "\u001b[J37          0.731836    0.730315              0.25           0.25                      0.171868      \n",
      "\u001b[J38          0.730315    0.7288                0.25           0.25                      0.175215      \n",
      "\u001b[J39          0.7288      0.727292              0.25           0.25                      0.178667      \n",
      "\u001b[J40          0.727292    0.72579               0.25           0.25                      0.182027      \n",
      "\u001b[J41          0.72579     0.724293              0.25           0.25                      0.185529      \n",
      "\u001b[J42          0.724293    0.722803              0.25           0.25                      0.18897       \n",
      "\u001b[J43          0.722803    0.721319              0.25           0.25                      0.192434      \n",
      "\u001b[J44          0.721319    0.719842              0.25           0.25                      0.195933      \n",
      "\u001b[J45          0.719842    0.71837               0.25           0.25                      0.199454      \n",
      "\u001b[J46          0.71837     0.716904              0.25           0.25                      0.202973      \n",
      "\u001b[J47          0.716904    0.715445              0.25           0.25                      0.206665      \n",
      "\u001b[J48          0.715445    0.713991              0.25           0.25                      0.211356      \n",
      "\u001b[J49          0.713991    0.712544              0.25           0.25                      0.215199      \n",
      "\u001b[J50          0.712544    0.711103              0.25           0.25                      0.221502      \n",
      "\u001b[J51          0.711103    0.709668              0.25           0.25                      0.225965      \n",
      "\u001b[J52          0.709668    0.708239              0.25           0.25                      0.229769      \n",
      "\u001b[J53          0.708239    0.706816              0.25           0.25                      0.235204      \n",
      "\u001b[J54          0.706816    0.7054                0.25           0.25                      0.238601      \n",
      "\u001b[J55          0.7054      0.703989              0.25           0.25                      0.242057      \n",
      "\u001b[J56          0.703989    0.702584              0.25           0.25                      0.24537       \n",
      "\u001b[J57          0.702584    0.701185              0.25           0.25                      0.249132      \n",
      "\u001b[J58          0.701185    0.699793              0.25           0.25                      0.252363      \n",
      "\u001b[J59          0.699793    0.698406              0.25           0.25                      0.256153      \n",
      "\u001b[J60          0.698406    0.697025              0.25           0.25                      0.259674      \n",
      "\u001b[J61          0.697025    0.695651              0.25           0.25                      0.26321       \n",
      "\u001b[J62          0.695651    0.694282              0.25           0.5                       0.266863      \n",
      "\u001b[J63          0.694282    0.692919              0.5            0.5                       0.270469      \n",
      "\u001b[J64          0.692919    0.691563              0.5            0.5                       0.273986      \n",
      "\u001b[J65          0.691563    0.690212              0.5            0.5                       0.27756       \n",
      "\u001b[J66          0.690212    0.688867              0.5            0.5                       0.281361      \n",
      "\u001b[J67          0.688867    0.687528              0.5            0.5                       0.284925      \n",
      "\u001b[J68          0.687528    0.686195              0.5            0.5                       0.288692      \n",
      "\u001b[J69          0.686195    0.684867              0.5            0.5                       0.292134      \n",
      "\u001b[J70          0.684867    0.683546              0.5            0.5                       0.295877      \n",
      "\u001b[J71          0.683546    0.68223               0.5            0.5                       0.299374      \n",
      "\u001b[J72          0.68223     0.680921              0.5            0.5                       0.303337      \n",
      "\u001b[J73          0.68092     0.679617              0.5            0.5                       0.306763      \n",
      "\u001b[J74          0.679617    0.678318              0.5            0.5                       0.310929      \n",
      "\u001b[J75          0.678318    0.677026              0.5            0.5                       0.314963      \n",
      "\u001b[J76          0.677026    0.675739              0.5            0.5                       0.318573      \n",
      "\u001b[J77          0.675739    0.674458              0.5            0.5                       0.322338      \n",
      "\u001b[J78          0.674458    0.673183              0.5            0.5                       0.326382      \n",
      "\u001b[J79          0.673183    0.671913              0.5            0.5                       0.330282      \n",
      "\u001b[J80          0.671913    0.67065               0.5            0.5                       0.334069      \n",
      "\u001b[J81          0.67065     0.669391              0.5            0.5                       0.338176      \n",
      "\u001b[J82          0.669391    0.668139              0.5            0.5                       0.341951      \n",
      "\u001b[J83          0.668139    0.666892              0.5            0.5                       0.346092      \n",
      "\u001b[J84          0.666892    0.665651              0.5            0.5                       0.350117      \n",
      "\u001b[J85          0.665651    0.664415              0.5            0.5                       0.35408       \n",
      "\u001b[J86          0.664415    0.663185              0.5            0.5                       0.357873      \n",
      "\u001b[J87          0.663185    0.66196               0.5            0.5                       0.361941      \n",
      "\u001b[J88          0.66196     0.660741              0.5            0.75                      0.365991      \n",
      "\u001b[J89          0.660741    0.659527              0.75           0.75                      0.37004       \n",
      "\u001b[J90          0.659527    0.658319              0.75           0.75                      0.374269      \n",
      "\u001b[J91          0.658319    0.657117              0.75           0.75                      0.378393      \n",
      "\u001b[J92          0.657117    0.65592               0.75           0.75                      0.382326      \n",
      "\u001b[J93          0.65592     0.654728              0.75           0.75                      0.38657       \n",
      "\u001b[J94          0.654728    0.653542              0.75           0.75                      0.39078       \n",
      "\u001b[J95          0.653542    0.652361              0.75           0.75                      0.394962      \n",
      "\u001b[J96          0.652361    0.651185              0.75           0.75                      0.399251      \n",
      "\u001b[J97          0.651185    0.650015              0.75           0.75                      0.403031      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J98          0.650015    0.648851              0.75           0.75                      0.407292      \n",
      "\u001b[J99          0.648851    0.647691              0.75           0.75                      0.413302      \n",
      "\u001b[J100         0.647691    0.646537              0.75           0.75                      0.419818      \n"
     ]
    }
   ],
   "source": [
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(2, 2)    #入力１，中間層１\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        y = self.l1(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "epoch = 100\n",
    "batch_size = 4\n",
    "\n",
    "#データの作成\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_train = np.array([0, 1, 1, 1], dtype=np.int32)\n",
    "train = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "test = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "print(\"train:\", train)\n",
    "\n",
    "#ニューラルネットワークの登録\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializer.load_npz(\"results/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "#イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batch_size)    #学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)    #評価用\n",
    "\n",
    "#アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "#トレーナの保存\n",
    "trainer = training.Trainer(updater, (epoch, \"epoch\"))\n",
    "\n",
    "#学習状況の保存や表示\n",
    "trainer.extend(extensions.LogReport())    #ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))    #エポック数の表示\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \n",
    "                                       \"main/accuracy\", \"validation/main/accuracy\", \"elapsed_time\"]))    #計算状態の表示\n",
    "#trainer.extend(extentions.dump_graph(\"main/loss\"))    #NNの構造\n",
    "#trainer.extend(extensions.PlotReport[\"main/loss\", \"validation/main/loss\"], \"epoch\", file_name=\"loss.png\")    #誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport[\"main/accuracy\", \"validation/main/accuracy\"], \"epoch\", file_name=\"accuracy.png\")    #精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, \"epoch\"))    #学習再開のためのスナップショットの出力\n",
    "\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)    #学習再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\")\n",
    "\n",
    "#学習開始\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNによる手書き文字認識　By　Chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.23529412 0.5882353  ... 0.         0.         0.        ]\n",
      " [0.         0.         0.23529412 ... 0.05882353 0.         0.        ]\n",
      " [0.         0.         0.5882353  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.3529412  ... 0.7058824  0.         0.        ]\n",
      " [0.         0.         0.23529412 ... 0.3529412  0.         0.        ]\n",
      " [0.         0.         0.23529412 ... 0.05882353 0.         0.        ]]\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           2.16851     1.99049               0.354          0.549167                  0.0442737     \n",
      "\u001b[J2           1.8061      1.5872                0.656429       0.723333                  0.0856669     \n",
      "\u001b[J3           1.33202     1.07138               0.796667       0.8075                    0.130251      \n",
      "\u001b[J4           0.890751    0.68953               0.852857       0.873333                  0.171197      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osaka/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/chainer/optimizers/adam.py:111: RuntimeWarning: invalid value encountered in sqrt\n",
      "  param.data -= hp.eta * (self.lr * m / (numpy.sqrt(vhat) + hp.eps) +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J5           0.585392    0.468361              0.879286       0.900833                  0.223074      \n",
      "\u001b[J6           0.416338    0.33789               0.903333       0.933333                  0.273515      \n",
      "\u001b[J7           0.322727    0.279738              0.926429       0.943333                  0.314577      \n",
      "\u001b[J8           0.261039    0.228505              0.937857       0.940833                  0.355586      \n",
      "\u001b[J9           0.223129    0.194014              0.953333       0.959167                  0.39916       \n",
      "\u001b[J10          0.190313    0.177999              0.955          0.955                     0.446948      \n",
      "\u001b[J11          0.166978    0.15626               0.964          0.9625                    0.493359      \n",
      "\u001b[J12          0.157149    0.142209              0.969286       0.973333                  0.534672      \n",
      "\u001b[J13          0.140012    0.133418              0.975714       0.966667                  0.575576      \n",
      "\u001b[J14          0.128455    0.125054              0.972          0.973333                  0.619111      \n",
      "\u001b[J15          0.122618    0.118794              0.979286       0.973333                  0.668154      \n",
      "\u001b[J16          0.105965    0.114319              0.981429       0.970833                  0.711641      \n",
      "\u001b[J17          0.10114     0.115881              0.982          0.970833                  0.765132      \n",
      "\u001b[J18          0.0997056   0.103934              0.981429       0.975833                  0.806169      \n",
      "\u001b[J19          0.0884217   0.102472              0.984          0.975833                  0.849946      \n",
      "\u001b[J20          0.0851501   0.104451              0.985          0.969167                  0.899401      \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(64, 100)    #入力６４、中間層１００\n",
    "            self.l2 = L.Linear(100, 100)    #中間１００，中間１００\n",
    "            self.l3 = L.Linear(100, 10)    #中間層１００、出力層１０\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "epoch = 20\n",
    "batch_size = 100\n",
    "\n",
    "#データの作成\n",
    "digits = load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "x_train = x_train.astype(np.float32) / 17\n",
    "x_test = x_test.astype(np.float32) / 17\n",
    "train = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "test = chainer.datasets.TupleDataset(x_test, y_test)\n",
    "\n",
    "print(x_train)\n",
    "\n",
    "#ニューラルネットワークの登録\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializer.load_npz(\"results/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "#イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batch_size)    #学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)    #評価用\n",
    "\n",
    "#アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "#トレーナの保存\n",
    "trainer = training.Trainer(updater, (epoch, \"epoch\"))\n",
    "\n",
    "#学習状況の保存や表示\n",
    "trainer.extend(extensions.LogReport())    #ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))    #エポック数の表示\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \n",
    "                                       \"main/accuracy\", \"validation/main/accuracy\", \"elapsed_time\"]))    #計算状態の表示\n",
    "#trainer.extend(extentions.dump_graph(\"main/loss\"))    #NNの構造\n",
    "#trainer.extend(extensions.PlotReport[\"main/loss\", \"validation/main/loss\"], \"epoch\", file_name=\"loss.png\")    #誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport[\"main/accuracy\", \"validation/main/accuracy\"], \"epoch\", file_name=\"accuracy.png\")    #精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, \"epoch\"))    #学習再開のためのスナップショットの出力\n",
    "\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)    #学習再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\")\n",
    "\n",
    "#学習開始\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNの実行\n",
    "↑との違いはネットワーク構造と学習データの違いだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osaka/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/chainer/optimizers/adam.py:111: RuntimeWarning: invalid value encountered in sqrt\n",
      "  param.data -= hp.eta * (self.lr * m / (numpy.sqrt(vhat) + hp.eps) +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           3.90026     1.55917               0.227333       0.473333                  0.381306      \n",
      "\u001b[J2           1.1175      0.731706              0.652143       0.8                       0.675543      \n",
      "\u001b[J3           0.530927    0.466744              0.839333       0.858333                  1.05082       \n",
      "\u001b[J4           0.339363    0.338975              0.896429       0.890833                  1.35138       \n",
      "\u001b[J5           0.247055    0.288681              0.927857       0.901667                  1.64578       \n",
      "\u001b[J6           0.198611    0.22781               0.946667       0.936667                  1.9644        \n",
      "\u001b[J7           0.151786    0.210724              0.964286       0.935833                  2.25859       \n",
      "\u001b[J8           0.129004    0.183146              0.965714       0.95                      2.56229       \n",
      "\u001b[J9           0.108717    0.168174              0.974667       0.955833                  2.91398       \n",
      "\u001b[J10          0.0885991   0.16837               0.980714       0.946667                  3.21028       \n",
      "\u001b[J11          0.0715609   0.146181              0.984          0.951667                  3.54024       \n",
      "\u001b[J12          0.0651233   0.142368              0.987857       0.956667                  3.84923       \n",
      "\u001b[J13          0.0539597   0.143106              0.992143       0.949167                  4.14631       \n",
      "\u001b[J14          0.0470503   0.126719              0.995333       0.961667                  4.45939       \n",
      "\u001b[J15          0.0419403   0.123361              0.995714       0.958333                  4.75521       \n",
      "\u001b[J16          0.0353376   0.118094              0.997857       0.959167                  5.05161       \n",
      "\u001b[J17          0.0313765   0.127038              0.998          0.955833                  5.3656        \n",
      "\u001b[J18          0.0290148   0.112638              0.998571       0.961667                  5.65897       \n",
      "\u001b[J19          0.0257826   0.123316              0.998          0.965833                  5.97995       \n",
      "\u001b[J20          0.0248601   0.111623              0.997143       0.965833                  6.27592       \n"
     ]
    }
   ],
   "source": [
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            #１層目の畳み込み層（入力チャンネル数（RGBなど）、出力チャンネル数、フィルタサイズ、ストライドサイズ、パディングサイズ）\n",
    "            self.conv1 = L.Convolution2D(1, 16, 3, stride=1, pad=1)\n",
    "            self.conv2 = L.Convolution2D(16, 64, 3, stride=1, pad=1)    #２層目の畳み込み層\n",
    "            self.l3 = L.Linear(256, 10)    #出力層。分類用\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        #max_pooling_2d(入力データ、フィルタサイズ、ストライドサイズ)\n",
    "        h1 = F.max_pooling_2d(F.relu(self.conv1(x)), ksize=2, stride=2)    #最大値プーリングは２×２、活性化関数にReLU\n",
    "        h2 = F.max_pooling_2d(F.relu(self.conv2(h1)), ksize=2, stride=2)\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "epoch = 20\n",
    "batch_size = 100\n",
    "\n",
    "#データの作成\n",
    "digits = load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "#print(\"original:\", x_train)\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), 1, 8, 8))    #1*8*8の行列に整形\n",
    "x_test = x_test.reshape((len(x_test), 1, 8, 8))\n",
    "\n",
    "x_train = (x_train).astype(np.float32)\n",
    "x_test = (x_test).astype(np.float32)\n",
    "train = chainer.datasets.TupleDataset(x_train, y_train)\n",
    "test = chainer.datasets.TupleDataset(x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "#ニューラルネットワークの登録\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializer.load_npz(\"results/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "#イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batch_size)    #学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batch_size, repeat=False, shuffle=False)    #評価用\n",
    "\n",
    "#アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "#トレーナの保存\n",
    "trainer = training.Trainer(updater, (epoch, \"epoch\"))\n",
    "\n",
    "#学習状況の保存や表示\n",
    "trainer.extend(extensions.LogReport())    #ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))    #エポック数の表示\n",
    "trainer.extend(extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \n",
    "                                       \"main/accuracy\", \"validation/main/accuracy\", \"elapsed_time\"]))    #計算状態の表示\n",
    "#trainer.extend(extentions.dump_graph(\"main/loss\"))    #NNの構造\n",
    "#trainer.extend(extensions.PlotReport[\"main/loss\", \"validation/main/loss\"], \"epoch\", file_name=\"loss.png\")    #誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport[\"main/accuracy\", \"validation/main/accuracy\"], \"epoch\", file_name=\"accuracy.png\")    #精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, \"epoch\"))    #学習再開のためのスナップショットの出力\n",
    "\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)    #学習再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\")\n",
    "\n",
    "#学習開始\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
