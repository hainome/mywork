{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 1 total reward 4\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 2 total reward 7\n",
      "\n",
      "0 1 0\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 3 total reward 8\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 0 0\n",
      "0 0 0\n",
      "episode: 4 total reward 7\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 5 total reward 7\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 6 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 7 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 8 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 9 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 10 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 11 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 12 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 13 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 14 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 15 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 16 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 17 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 18 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 19 total reward 9\n",
      "\n",
      "0 0 0\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "episode: 20 total reward 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "def random_action():\n",
    "    \"\"\"\n",
    "    actionはランダムに行われる\n",
    "    \"\"\"\n",
    "    return np.random.choice([0, 1])\n",
    "\n",
    "def get_action(next_state, episode):\n",
    "    epsilon = 0.5 * (1 / (episode + 1))    #徐々に最適な行動のみ取るようになる（ε-greedy法）\n",
    "    \n",
    "    if epsilon <= np.random.uniform(0, 1):    #np.random.uniform(0,1) ~０から１までのランダムなfloat\n",
    "        a = np.where(q_table[next_state]==q_table[next_state].max())[0]\n",
    "        next_action = np.random.choice(a)\n",
    "    else:\n",
    "        next_action = random_action()\n",
    "        \n",
    "    return next_action\n",
    "\n",
    "def step(state, action):\n",
    "    reward = 0\n",
    "    if state == 0:\n",
    "        if action == 0:\n",
    "            state = 1\n",
    "        else:\n",
    "            state = 0\n",
    "            \n",
    "    else:\n",
    "        if action == 0:\n",
    "            state = 0\n",
    "        else:\n",
    "            state = 1\n",
    "            reward = 1\n",
    "    return state, reward\n",
    "\n",
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.9\n",
    "    alpha = 0.5\n",
    "    next_maxQ = max(q_table[next_state])\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * next_maxQ)\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "max_number_of_steps = 10    #1試行のstep数\n",
    "num_episodes = 20    #総試行回数\n",
    "q_table = np.zeros((2, 2))\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(max_number_of_steps):\n",
    "        action = get_action(state, episode)    #a_{t-1}\n",
    "        next_state, reward = step(state, action)\n",
    "        print(state, action, reward)\n",
    "        episode_reward += reward\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state)\n",
    "        state = next_state\n",
    "    \n",
    "    print(\"episode: {} total reward {}\".format(episode+1, episode_reward))\n",
    "    print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gymによる倒立振子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode:0\n",
      "reward:-165.0\n",
      "episode:1\n",
      "reward:-184.0\n",
      "episode:2\n",
      "reward:-171.0\n",
      "episode:3\n",
      "reward:-180.0\n",
      "episode:4\n",
      "reward:-182.0\n",
      "episode:5\n",
      "reward:-176.0\n",
      "episode:6\n",
      "reward:-147.0\n",
      "episode:7\n",
      "reward:-133.0\n",
      "episode:8\n",
      "reward:-173.0\n",
      "episode:9\n",
      "reward:-126.0\n",
      "episode:10\n",
      "reward:-179.0\n",
      "episode:11\n",
      "reward:-123.0\n",
      "episode:12\n",
      "reward:-168.0\n",
      "episode:13\n",
      "reward:-93.0\n",
      "episode:14\n",
      "reward:-183.0\n",
      "episode:15\n",
      "reward:-129.0\n",
      "episode:16\n",
      "reward:-123.0\n",
      "episode:17\n",
      "reward:-55.0\n",
      "episode:18\n",
      "reward:-162.0\n",
      "episode:19\n",
      "reward:-119.0\n",
      "episode:20\n",
      "reward:-173.0\n",
      "episode:21\n",
      "reward:-137.0\n",
      "episode:22\n",
      "reward:-91.0\n",
      "episode:23\n",
      "reward:-107.0\n",
      "episode:24\n",
      "reward:-96.0\n",
      "episode:25\n",
      "reward:-120.0\n",
      "episode:26\n",
      "reward:-135.0\n",
      "episode:27\n",
      "reward:-143.0\n",
      "episode:28\n",
      "reward:-99.0\n",
      "episode:29\n",
      "reward:-165.0\n",
      "episode:30\n",
      "reward:-133.0\n",
      "episode:31\n",
      "reward:-114.0\n",
      "episode:32\n",
      "reward:-149.0\n",
      "episode:33\n",
      "reward:-154.0\n",
      "episode:34\n",
      "reward:-141.0\n",
      "episode:35\n",
      "reward:-132.0\n",
      "episode:36\n",
      "reward:-149.0\n",
      "episode:37\n",
      "reward:-183.0\n",
      "episode:38\n",
      "reward:-157.0\n",
      "episode:39\n",
      "reward:-100.0\n",
      "episode:40\n",
      "reward:-119.0\n",
      "episode:41\n",
      "reward:-166.0\n",
      "episode:42\n",
      "reward:-143.0\n",
      "episode:43\n",
      "reward:-132.0\n",
      "episode:44\n",
      "reward:-172.0\n",
      "episode:45\n",
      "reward:-123.0\n",
      "episode:46\n",
      "reward:-177.0\n",
      "episode:47\n",
      "reward:-182.0\n",
      "episode:48\n",
      "reward:-72.0\n",
      "episode:49\n",
      "reward:-160.0\n",
      "episode:50\n",
      "reward:-48.0\n",
      "episode:51\n",
      "reward:-34.0\n",
      "episode:52\n",
      "reward:-137.0\n",
      "episode:53\n",
      "reward:-115.0\n",
      "episode:54\n",
      "reward:-126.0\n",
      "episode:55\n",
      "reward:-115.0\n",
      "episode:56\n",
      "reward:-144.0\n",
      "episode:57\n",
      "reward:-89.0\n",
      "episode:58\n",
      "reward:200.0\n",
      "episode:59\n",
      "reward:-50.0\n",
      "episode:60\n",
      "reward:-156.0\n",
      "episode:61\n",
      "reward:-81.0\n",
      "episode:62\n",
      "reward:-89.0\n",
      "episode:63\n",
      "reward:-102.0\n",
      "episode:64\n",
      "reward:-132.0\n",
      "episode:65\n",
      "reward:-101.0\n",
      "episode:66\n",
      "reward:-72.0\n",
      "episode:67\n",
      "reward:-88.0\n",
      "episode:68\n",
      "reward:-95.0\n",
      "episode:69\n",
      "reward:-30.0\n",
      "episode:70\n",
      "reward:-13.0\n",
      "episode:71\n",
      "reward:-120.0\n",
      "episode:72\n",
      "reward:-131.0\n",
      "episode:73\n",
      "reward:-73.0\n",
      "episode:74\n",
      "reward:-103.0\n",
      "episode:75\n",
      "reward:200.0\n",
      "episode:76\n",
      "reward:200.0\n",
      "episode:77\n",
      "reward:-98.0\n",
      "episode:78\n",
      "reward:-91.0\n",
      "episode:79\n",
      "reward:-84.0\n",
      "episode:80\n",
      "reward:200.0\n",
      "episode:81\n",
      "reward:-15.0\n",
      "episode:82\n",
      "reward:200.0\n",
      "episode:83\n",
      "reward:-77.0\n",
      "episode:84\n",
      "reward:200.0\n",
      "episode:85\n",
      "reward:-56.0\n",
      "episode:86\n",
      "reward:-27.0\n",
      "episode:87\n",
      "reward:200.0\n",
      "episode:88\n",
      "reward:-40.0\n",
      "episode:89\n",
      "reward:200.0\n",
      "episode:90\n",
      "reward:200.0\n",
      "episode:91\n",
      "reward:200.0\n",
      "episode:92\n",
      "reward:-69.0\n",
      "episode:93\n",
      "reward:200.0\n",
      "episode:94\n",
      "reward:-58.0\n",
      "episode:95\n",
      "reward:200.0\n",
      "episode:96\n",
      "reward:-116.0\n",
      "episode:97\n",
      "reward:-141.0\n",
      "episode:98\n",
      "reward:-127.0\n",
      "episode:99\n",
      "reward:200.0\n",
      "episode:100\n",
      "reward:-67.0\n",
      "episode:101\n",
      "reward:200.0\n",
      "episode:102\n",
      "reward:200.0\n",
      "episode:103\n",
      "reward:-53.0\n",
      "episode:104\n",
      "reward:-100.0\n",
      "episode:105\n",
      "reward:-107.0\n",
      "episode:106\n",
      "reward:-8.0\n",
      "episode:107\n",
      "reward:-127.0\n",
      "episode:108\n",
      "reward:-35.0\n",
      "episode:109\n",
      "reward:-48.0\n",
      "episode:110\n",
      "reward:-6.0\n",
      "episode:111\n",
      "reward:-140.0\n",
      "episode:112\n",
      "reward:-164.0\n",
      "episode:113\n",
      "reward:-96.0\n",
      "episode:114\n",
      "reward:200.0\n",
      "episode:115\n",
      "reward:200.0\n",
      "episode:116\n",
      "reward:-54.0\n",
      "episode:117\n",
      "reward:200.0\n",
      "episode:118\n",
      "reward:200.0\n",
      "episode:119\n",
      "reward:200.0\n",
      "episode:120\n",
      "reward:200.0\n",
      "episode:121\n",
      "reward:-139.0\n",
      "episode:122\n",
      "reward:-18.0\n",
      "episode:123\n",
      "reward:-45.0\n",
      "episode:124\n",
      "reward:-113.0\n",
      "episode:125\n",
      "reward:-140.0\n",
      "episode:126\n",
      "reward:-128.0\n",
      "episode:127\n",
      "reward:200.0\n",
      "episode:128\n",
      "reward:-135.0\n",
      "episode:129\n",
      "reward:-82.0\n",
      "episode:130\n",
      "reward:-60.0\n",
      "episode:131\n",
      "reward:-59.0\n",
      "episode:132\n",
      "reward:200.0\n",
      "episode:133\n",
      "reward:200.0\n",
      "episode:134\n",
      "reward:200.0\n",
      "episode:135\n",
      "reward:-33.0\n",
      "episode:136\n",
      "reward:200.0\n",
      "episode:137\n",
      "reward:200.0\n",
      "episode:138\n",
      "reward:-35.0\n",
      "episode:139\n",
      "reward:200.0\n",
      "episode:140\n",
      "reward:200.0\n",
      "episode:141\n",
      "reward:-5.0\n",
      "episode:142\n",
      "reward:-127.0\n",
      "episode:143\n",
      "reward:200.0\n",
      "episode:144\n",
      "reward:-96.0\n",
      "episode:145\n",
      "reward:-31.0\n",
      "episode:146\n",
      "reward:-34.0\n",
      "episode:147\n",
      "reward:-150.0\n",
      "episode:148\n",
      "reward:-155.0\n",
      "episode:149\n",
      "reward:-147.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0a3176e4b665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number_of_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_number_of_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "def digitize_state(observation):\n",
    "    p, a, v, w = observation\n",
    "    d = num_digitized\n",
    "    pn = np.digitize(p, np.linspace(-2.4, 2.4, d+1)[1:-1])\n",
    "    vn = np.digitize(v, np.linspace(-3.0, 3.0, d+1)[1:-1])\n",
    "    an = np.digitize(a, np.linspace(-0.5, 0.5, d+1)[1:-1])\n",
    "    wn = np.digitize(w, np.linspace(-2.0, 2.0, d+1)[1:-1])\n",
    "    \n",
    "    return pn + vn*d + an*d + wn*d**3\n",
    "\n",
    "def get_action(next_state, episode):\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        a = np.where(q_table[next_state]==q_table[next_state].max())[0]\n",
    "        next_action = np.random.choice(a)\n",
    "    else:\n",
    "        next_action = random_action()\n",
    "        \n",
    "    return next_action\n",
    "\n",
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.9\n",
    "    alpha = 0.5\n",
    "    next_maxQ = max(q_table[next_state])\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * next_maxQ)\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "max_number_of_steps = 200    #１試行のStep数\n",
    "num_episodes = 1000\n",
    "num_digitized = 6    #振り子の位置の分割数\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(num_digitized**4, env.action_space.n))\n",
    "#q_table = np.loadtxt(\"Qvalue.txt\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    #環境の初期化\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation)\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(max_number_of_steps):\n",
    "        if episode % 10 == 0:\n",
    "            env.render()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done and t < max_number_of_steps - 1:\n",
    "            reward -= max_number_of_steps    #倒れたら罰則\n",
    "        episode_reward += reward\n",
    "        next_state = digitize_state(observation)    #t+1の観測状態を、離散地に変換（棒の角度）\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state)\n",
    "        action = get_action(next_state, episode)    #a_{t+1}\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    print(\"episode:{}\\nreward:{}\".format(episode, episode_reward))\n",
    "np.savetxt(\"Qtable.txt\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
