{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQNをねずみ学習問題に適用してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osaka/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "######skinner_DQN.py##########\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "import chainerrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "episode: 1    total reward: 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "episode: 2    total reward: 2\n",
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "episode: 3    total reward: 1\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 4    total reward: 2\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "episode: 5    total reward: 2\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 6    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 7    total reward: 2\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "episode: 8    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 9    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "episode: 10    total reward: 2\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 11    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 12    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 13    total reward: 4\n",
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 14    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 15    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 16    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 17    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "episode: 18    total reward: 2\n",
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 19    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 20    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 21    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 22    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 23    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 24    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 25    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 26    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 27    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "episode: 28    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 0 0\n",
      "episode: 29    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 30    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 31    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 32    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 33    total reward: 4\n",
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 34    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 0 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 35    total reward: 2\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 36    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 37    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 38    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 39    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 40    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 41    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 42    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 43    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 44    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 45    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 46    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 47    total reward: 4\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 48    total reward: 4\n",
      "[0] 1 0\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 49    total reward: 3\n",
      "[0] 0 0\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "[1] 1 1\n",
      "episode: 50    total reward: 4\n"
     ]
    }
   ],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=2):\n",
    "        super(QFunction, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l3 = L.Linear(n_hidden_channels, n_actions)\n",
    "            \n",
    "    def __call__(self, x, test=False):\n",
    "        h1 = F.tanh(self.l1(x))\n",
    "        h2 = F.tanh(self.l2(h1))\n",
    "        y = chainerrl.action_value.DiscreteActionValue(self.l3(h2))\n",
    "        return y\n",
    "    \n",
    "def random_action():\n",
    "    return np.random.choice([0, 1])\n",
    "\n",
    "def step(state, action):\n",
    "    reward = 0\n",
    "    if state == 0:\n",
    "        if action == 0:\n",
    "            state = 1\n",
    "        else:\n",
    "            state = 0\n",
    "            \n",
    "    else:\n",
    "        if action == 0:\n",
    "            state = 0\n",
    "        else:\n",
    "            state = 1\n",
    "            reward += 1\n",
    "            \n",
    "    return np.array([state]), reward\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5\n",
    "max_number_of_steps = 5    #1試行のステップ数\n",
    "num_episodes = 50\n",
    "\n",
    "q_func = QFunction(1, 2)\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func)\n",
    "explorer = chainerrl.explorers.LinearDecayEpsilonGreedy(start_epsilon=1.0, \n",
    "                                        end_epsilon=0.1, decay_steps=num_episodes,\n",
    "                                        random_action_func=random_action)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10**6)\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "agent = chainerrl.agents.DQN(\n",
    "                            q_func, optimizer, replay_buffer, \n",
    "                            gamma, explorer, replay_start_size=500,\n",
    "                            update_interval=1, target_update_interval=100,\n",
    "                            phi=phi)\n",
    "#agent.load(\"agent\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = np.array([0])\n",
    "    R = 0\n",
    "    reward = 0\n",
    "    done = True\n",
    "    \n",
    "    for t in range(max_number_of_steps):\n",
    "        action = agent.act_and_train(state, reward)\n",
    "        next_state, reward = step(state, action)\n",
    "        print(state, action, reward)\n",
    "        R += reward    #報酬を追加\n",
    "        state = next_state\n",
    "    agent.stop_episode_and_train(state, reward, done)\n",
    "    \n",
    "    print(\"episode: {}    total reward: {}\".format(episode+1, R))\n",
    "agent.save(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gymによる倒立振子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import myenv\n",
    "import numpy as np\n",
    "import gym    #倒立振子の実行環境\n",
    "from gym import wrappers    #Gymの画像保存\n",
    "import time\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
